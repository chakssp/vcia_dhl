# Alert Rules Configuration
# ML Confidence Workflow System
# 
# Comprehensive alerting rules for production monitoring

groups:
  - name: confidence_alerts
    interval: 30s
    rules:
      # Confidence below target
      - alert: ConfidenceBelowTarget
        expr: avg(ml_confidence_score) < 0.85
        for: 5m
        labels:
          severity: warning
          team: ml-team
        annotations:
          summary: "ML Confidence below 85% target"
          description: "Average confidence score is {{ $value | humanizePercentage }} (target: 85%)"
          runbook_url: "https://wiki.company.com/runbooks/ml-confidence-low"
          
      # Critical confidence drop
      - alert: ConfidenceCritical
        expr: avg(ml_confidence_score) < 0.75
        for: 2m
        labels:
          severity: critical
          team: ml-team
          page: true
        annotations:
          summary: "ML Confidence critically low"
          description: "Average confidence has dropped to {{ $value | humanizePercentage }}"
          impact: "Analysis quality severely degraded"
          action: "Investigate algorithm performance and data quality immediately"
          
      # Confidence variance too high
      - alert: ConfidenceVarianceHigh
        expr: stddev(ml_confidence_score) > 0.1
        for: 10m
        labels:
          severity: warning
          team: ml-team
        annotations:
          summary: "High confidence variance detected"
          description: "Confidence variance is {{ $value | humanize }} (threshold: 0.1)"
          
  - name: performance_alerts
    interval: 30s
    rules:
      # High latency
      - alert: ProcessingLatencyHigh
        expr: histogram_quantile(0.95, ml_processing_duration_seconds) > 2
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Processing latency exceeds 2 seconds"
          description: "P95 latency is {{ $value | humanizeDuration }}"
          
      # Very high latency
      - alert: ProcessingLatencyCritical
        expr: histogram_quantile(0.99, ml_processing_duration_seconds) > 5
        for: 2m
        labels:
          severity: critical
          team: platform
          page: true
        annotations:
          summary: "Critical processing latency"
          description: "P99 latency is {{ $value | humanizeDuration }}"
          impact: "User experience severely impacted"
          
      # Low throughput
      - alert: ThroughputLow
        expr: rate(ml_items_processed_total[5m]) < 10
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Processing throughput below threshold"
          description: "Current rate: {{ $value | humanize }} items/sec (threshold: 10)"
          
  - name: cache_alerts
    interval: 30s
    rules:
      # Cache hit rate low
      - alert: CacheHitRateLow
        expr: |
          (
            sum(rate(ml_cache_hits_total[5m]))
            /
            (sum(rate(ml_cache_hits_total[5m])) + sum(rate(ml_cache_misses_total[5m])))
          ) < 0.90
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Cache hit rate below 90%"
          description: "Current hit rate: {{ $value | humanizePercentage }}"
          
      # Cache evictions high
      - alert: CacheEvictionsHigh
        expr: rate(ml_cache_evictions_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High cache eviction rate"
          description: "Evicting {{ $value | humanize }} items/sec"
          
  - name: worker_alerts
    interval: 30s
    rules:
      # Worker pool exhausted
      - alert: WorkerPoolExhausted
        expr: ml_workers_available == 0
        for: 1m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "No available workers in pool"
          description: "All workers are busy, queuing may occur"
          
      # Worker errors high
      - alert: WorkerErrorsHigh
        expr: rate(ml_worker_errors_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High worker error rate"
          description: "Worker errors at {{ $value | humanize }} per second"
          
  - name: convergence_alerts
    interval: 30s
    rules:
      # Slow convergence
      - alert: ConvergenceSlow
        expr: avg(ml_convergence_iterations) > 6
        for: 15m
        labels:
          severity: warning
          team: ml-team
        annotations:
          summary: "Convergence taking too many iterations"
          description: "Average iterations: {{ $value | humanize }} (threshold: 6)"
          
      # Failed convergence
      - alert: ConvergenceFailed
        expr: rate(ml_convergence_failures_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          team: ml-team
        annotations:
          summary: "High convergence failure rate"
          description: "{{ $value | humanizePercentage }} of analyses failing to converge"
          
  - name: resource_alerts
    interval: 30s
    rules:
      # High memory usage
      - alert: MemoryUsageHigh
        expr: ml_memory_usage_bytes / ml_memory_limit_bytes > 0.85
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Memory usage above 85%"
          description: "Current usage: {{ $value | humanizePercentage }} of limit"
          
      # Critical memory usage
      - alert: MemoryUsageCritical
        expr: ml_memory_usage_bytes / ml_memory_limit_bytes > 0.95
        for: 2m
        labels:
          severity: critical
          team: platform
          page: true
        annotations:
          summary: "Memory usage critical"
          description: "Using {{ $value | humanizePercentage }} of available memory"
          action: "Scale up or optimize memory usage immediately"
          
      # High CPU usage
      - alert: CPUUsageHigh
        expr: avg(ml_cpu_usage_percent) > 80
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "CPU usage above 80%"
          description: "Average CPU: {{ $value | humanize }}%"
          
  - name: error_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: ErrorRateHigh
        expr: |
          (
            sum(rate(ml_requests_failed_total[5m]))
            /
            sum(rate(ml_requests_total[5m]))
          ) > 0.01
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Error rate above 1%"
          description: "Current error rate: {{ $value | humanizePercentage }}"
          
      # Critical error rate
      - alert: ErrorRateCritical
        expr: |
          (
            sum(rate(ml_requests_failed_total[5m]))
            /
            sum(rate(ml_requests_total[5m]))
          ) > 0.05
        for: 2m
        labels:
          severity: critical
          team: platform
          page: true
        annotations:
          summary: "Critical error rate"
          description: "Error rate at {{ $value | humanizePercentage }}"
          impact: "Service reliability compromised"
          
  - name: data_quality_alerts
    interval: 60s
    rules:
      # Invalid input data
      - alert: InvalidInputData
        expr: rate(ml_invalid_inputs_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          team: data
        annotations:
          summary: "High rate of invalid input data"
          description: "Receiving {{ $value | humanize }} invalid inputs/sec"
          
      # Missing embeddings
      - alert: MissingEmbeddings
        expr: rate(ml_missing_embeddings_total[5m]) > 0.05
        for: 10m
        labels:
          severity: warning
          team: ml-team
        annotations:
          summary: "Files missing embeddings"
          description: "{{ $value | humanize }} files/sec lacking embeddings"
          
  - name: deployment_alerts
    interval: 30s
    rules:
      # Canary deployment issues
      - alert: CanaryDeploymentFailing
        expr: |
          (
            ml_canary_confidence_score < 0.80
            or
            ml_canary_error_rate > 0.05
          )
        for: 5m
        labels:
          severity: critical
          team: platform
          deployment: canary
        annotations:
          summary: "Canary deployment showing issues"
          description: "Canary metrics degraded, automatic rollback may trigger"
          
      # Version mismatch
      - alert: VersionMismatch
        expr: count(count by (version) (ml_app_info)) > 1
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Multiple versions running simultaneously"
          description: "Detected {{ $value }} different versions in production"

# Alert notification routing
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    # Critical alerts - immediate page
    - match:
        severity: critical
      receiver: pagerduty
      continue: true
      
    # ML team alerts
    - match:
        team: ml-team
      receiver: ml-team-slack
      
    # Platform team alerts  
    - match:
        team: platform
      receiver: platform-slack
      
    # Data team alerts
    - match:
        team: data
      receiver: data-team-email

# Notification receivers
receivers:
  - name: default
    email_configs:
      - to: 'alerts@company.com'
        from: 'ml-confidence@company.com'
        headers:
          Subject: 'ML Confidence Alert: {{ .GroupLabels.alertname }}'
          
  - name: pagerduty
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
        details:
          severity: '{{ .GroupLabels.severity }}'
          impact: '{{ .CommonAnnotations.impact }}'
          
  - name: ml-team-slack
    slack_configs:
      - api_url: '${SLACK_ML_WEBHOOK}'
        channel: '#ml-alerts'
        title: 'ML Confidence Alert'
        text: '{{ .CommonAnnotations.summary }}'
        
  - name: platform-slack
    slack_configs:
      - api_url: '${SLACK_PLATFORM_WEBHOOK}'
        channel: '#platform-alerts'
        title: 'Platform Alert'
        text: '{{ .CommonAnnotations.summary }}'
        
  - name: data-team-email
    email_configs:
      - to: 'data-team@company.com'
        from: 'ml-confidence@company.com'

# Inhibition rules
inhibit_rules:
  # Inhibit warnings when critical alert is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']