Na era pré-IA, os motores de busca funcionavam primariamente com base em dois pilares: **rastreamento (crawling)** e **ranqueamento por autoridade**.

1. **Wikipedia - Rastreamento e Indexação por Palavras-Chave:** Robôs, conhecidos como `crawlers` ou `spiders`, navegavam incessantemente pela internet, saltando de link em link para descobrir novas páginas. Ao encontrar uma página, eles analisavam seu conteúdo e o indexavam com base nas palavras-chave presentes no texto, títulos e metadados. A Wikipédia, para esses robôs, era apenas mais um site gigante, embora muito bem interligado.

2. **Google - PageRank e a Autoridade Externa:** O grande diferencial de buscadores como o Google foi o algoritmo `PageRank`. Ele não media apenas se uma palavra-chave aparecia em uma página, mas determinava a **relevância e a autoridade** dessa página com base na quantidade e na qualidade dos links que ela recebia de outros sites. Um link de um site de alta autoridade para um artigo da Wikipédia funcionava como um "voto de confiança", aumentando a probabilidade de aquele artigo aparecer no topo dos resultados de busca para um determinado tópico. A relevância era, em grande parte, uma medida de popularidade e confiança externa.

3. **Elasticsearch - 2004/10/ Release Launch 2012 - Estrutura Interna e Conteúdo:** A busca da Wikipédia explorava sua própria estrutura hipertextual. A densidade de links internos para um artigo específico e sua categorização hierárquica também eram sinais de sua importância dentro do ecossistema da enciclopédia. Ferramentas de busca mais avançadas, como a `CirrusSearch` (baseada em `Elasticsearch`) que foi implementada posteriormente, permitiram buscas mais sofisticadas no texto-fonte (`wikitext`), mas o princípio fundamental permaneceu: a lógica era a da própria enciclopédia.
   