# üîß Troubleshooting: Resposta Vazia do Ollama

## üêõ Descri√ß√£o do Problema

Durante os testes de homologa√ß√£o do sistema de IA, o modelo Qwen3 14B est√° retornando respostas vazias (`{}`) quando deveria gerar an√°lises estruturadas dos arquivos.

**Data de Identifica√ß√£o**: 15/01/2025  
**Sprint**: 1.3 - An√°lise com IA  
**Severidade**: ALTA  
**Status**: EM INVESTIGA√á√ÉO  

---

## üìä Sintomas Observados

1. **Resposta Vazia**
   ```javascript
   üìä Resposta bruta: {}...
   ‚úÖ An√°lise parseada com sucesso: {}
   ```

2. **Tempo de Resposta Anormal**
   - Esperado: 2-5 segundos para an√°lise completa
   - Observado: 0.3 segundos
   - Indicativo: Modelo n√£o est√° processando o prompt

3. **Normaliza√ß√£o Padr√£o**
   ```javascript
   {
     analysisType: 'Aprendizado Geral',
     moments: [],
     categories: [],
     summary: '',
     relevanceScore: 0.5
   }
   ```

---

## üîç An√°lise de Causas Poss√≠veis

### 1. **Formato de Prompt Incorreto**
- Ollama pode estar esperando formato diferente
- Par√¢metro `format: "json"` pode estar for√ßando resposta vazia

### 2. **Par√¢metros de Gera√ß√£o Insuficientes**
- `num_predict: 500` pode ser muito baixo
- Falta de par√¢metro `num_ctx` (contexto)

### 3. **Modelo N√£o Carregado**
- Qwen3 pode n√£o estar totalmente carregado na mem√≥ria
- Primeira chamada ap√≥s inicializa√ß√£o

### 4. **Incompatibilidade de Formato**
- System prompt + user prompt concatenados
- Modelo pode esperar formato espec√≠fico

---

## üõ†Ô∏è Solu√ß√µes Propostas

### Solu√ß√£o 1: Ajustar Par√¢metros de Gera√ß√£o
```javascript
const response = await fetch('http://127.0.0.1:11434/api/generate', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        model: 'qwen3:14b',
        prompt: prompt,
        stream: false,
        // Remover format: "json" temporariamente
        options: {
            temperature: 0.7,
            num_predict: 2000,    // Aumentar de 500
            num_ctx: 4096,        // Adicionar contexto
            repeat_penalty: 1.1,   // Evitar loops
            stop: ["```", "\n\n\n"] // Paradas expl√≠citas
        }
    })
});
```

### Solu√ß√£o 2: Reformatar Prompt
```javascript
// Ao inv√©s de concatenar system + user
const formattedPrompt = `### Instru√ß√£o:
${systemPrompt}

### Entrada:
${userPrompt}

### Resposta:
`;
```

### Solu√ß√£o 3: Teste de Debug Direto
```javascript
async function debugOllamaResponse() {
    console.log("üîç DEBUG: Testando resposta detalhada do Ollama");
    
    // Prompt mais simples e direto
    const testPrompt = `Analise o seguinte texto e retorne um JSON:

Texto: "Esta √© uma decis√£o importante sobre arquitetura de software."

Retorne APENAS isto:
{
    "analysisType": "Momento Decisivo",
    "summary": "Decis√£o sobre arquitetura"
}`;

    const response = await fetch('http://127.0.0.1:11434/api/generate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
            model: 'qwen3:14b',
            prompt: testPrompt,
            stream: false,
            options: {
                temperature: 0.3,
                num_predict: 1000,
                seed: 123  // Para reprodutibilidade
            }
        })
    });

    const result = await response.json();
    console.log("Resposta completa:", result);
    
    // Verificar campos da resposta
    console.log("Campo 'response':", result.response);
    console.log("Campo 'done':", result.done);
    console.log("Campo 'context':", result.context?.length);
    console.log("Campo 'total_duration':", result.total_duration);
    
    return result;
}
```

### Solu√ß√£o 4: Testar com Chat API
```javascript
// Usar /api/chat ao inv√©s de /api/generate
const response = await fetch('http://127.0.0.1:11434/api/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        model: 'qwen3:14b',
        messages: [
            {
                role: "system",
                content: systemPrompt
            },
            {
                role: "user",
                content: userPrompt
            }
        ],
        stream: false,
        format: "json"
    })
});
```

---

## üìã Checklist de Diagn√≥stico

### 1. Verificar Servidor Ollama
```bash
# Verificar se est√° rodando
curl http://127.0.0.1:11434/api/tags

# Verificar logs do servidor
journalctl -u ollama -f

# Ou se iniciado manualmente
# Verificar terminal onde ollama serve est√° rodando
```

### 2. Testar Modelo Diretamente
```bash
# Via CLI
ollama run qwen3:14b "Responda com um JSON simples: {status: 'ok'}"

# Verificar se modelo est√° corrompido
ollama list
ollama pull qwen3:14b --force  # Re-baixar se necess√°rio
```

### 3. Validar com Outros Modelos
```javascript
// Testar com DeepSeek-R1
KC.AIAPIManager.providers.ollama.defaultModel = 'hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_0';
```

---

## üöÄ Plano de A√ß√£o

### Fase 1: Diagn√≥stico Imediato
1. Executar fun√ß√£o `debugOllamaResponse()` no console
2. Verificar logs do servidor Ollama
3. Testar prompt simples via CLI

### Fase 2: Ajustes de Configura√ß√£o
1. Remover `format: "json"` temporariamente
2. Aumentar `num_predict` para 2000+
3. Adicionar `num_ctx: 4096`

### Fase 3: Reformula√ß√£o de Prompts
1. Usar Chat API ao inv√©s de Generate API
2. Simplificar estrutura do prompt
3. Adicionar exemplos no prompt

### Fase 4: Valida√ß√£o
1. Testar com m√∫ltiplos arquivos
2. Comparar resultados entre modelos
3. Medir tempos de resposta

---

## üí° Workaround Tempor√°rio

Enquanto investigamos, usar fallback para OpenAI/Gemini:
```javascript
// Em AIAPIManager
if (response.response === '{}' && this.activeProvider === 'ollama') {
    console.warn('Ollama retornou vazio, tentando fallback...');
    return this._callWithFallback(prompt, 'openai');
}
```

---

## üìö Refer√™ncias

- [Ollama API Documentation](https://github.com/jmorganca/ollama/blob/main/docs/api.md)
- [Qwen3 Model Card](https://huggingface.co/Qwen/Qwen3-14B)
- [Troubleshooting Guide Ollama](https://github.com/jmorganca/ollama/blob/main/docs/troubleshooting.md)

---

**√öltima Atualiza√ß√£o**: 15/01/2025  
**Pr√≥xima Revis√£o**: Ap√≥s testes de debug