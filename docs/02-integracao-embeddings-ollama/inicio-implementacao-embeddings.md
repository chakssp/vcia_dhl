# üöÄ Sprint Fase 2 - In√≠cio da Implementa√ß√£o de Embeddings

**Data**: 17/01/2025  
**Sprint**: FASE 2 - Funda√ß√£o Sem√¢ntica  
**Status**: üöß EM ANDAMENTO  

## üìã O que foi implementado

### ‚úÖ EmbeddingService.js (Conclu√≠do)
- **Localiza√ß√£o**: `/js/services/EmbeddingService.js`
- **Linhas**: 410 linhas de c√≥digo documentado
- **Funcionalidades**:
  - ‚úÖ Gera√ß√£o de embeddings com Ollama (local)
  - ‚úÖ Fallback para OpenAI se necess√°rio
  - ‚úÖ Cache inteligente em IndexedDB
  - ‚úÖ Processamento em batch
  - ‚úÖ C√°lculo de similaridade coseno
  - ‚úÖ Enriquecimento de contexto (categorias)
  - ‚úÖ Estat√≠sticas de uso

### ‚úÖ Integra√ß√£o no Sistema
- Adicionado ao `app.js` no array de componentes
- Inclu√≠do no `index.html` na se√ß√£o de Scripts Services
- Auto-registro no namespace KC

### ‚úÖ P√°gina de Testes
- **Arquivo**: `/test/test-embedding-service.html`
- **Funcionalidades de teste**:
  - Verifica√ß√£o de status do servi√ßo
  - Teste de conex√£o com Ollama
  - Gera√ß√£o de embedding individual
  - C√°lculo de similaridade entre textos
  - Processamento em batch
  - Visualiza√ß√£o de estat√≠sticas

## üèóÔ∏è Arquitetura Implementada

```javascript
// Configura√ß√£o pragm√°tica - usa VPS via Tailscale
const API_CONFIG = {
    // Ollama local para embeddings r√°pidos
    // docker exec ollama ollama list
          // NAME                                                    ID              SIZE      MODIFIED
          // nomic-embed-text:latest                                 0a109f422b47    274 MB    5 days ago
          // hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_0       55f8d29f71fb    8.7 GB    5 days ago
          // hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL    3627dcaf61c2    10 GB     9 days ago
          // hf.co/unsloth/Qwen3-14B-GGUF:Q6_K_XL                    b0069ab3940f    13 GB     9 days ago
          // qwen3:14b                                               bdbd181c33f2    9.3 GB    11 days ago
    ollama: {
        url: 'http://localhost:11434',
        model: 'nomic-embed-text'
    },
    // VPS com Tailscale para armazenamento
    // Endere√ßo Servidor VPS na Rede Tailscale
    postgres: '100.64.236.43:5432',
    qdrant: 'https://qdr.vcia.com.br:6333',
    n8n: 'http://n8n.vcia.com.br:5678'
}
```

## üß™ Como Testar

### 1. Verificar se Ollama est√° instalado
```bash
# No terminal
curl http://localhost:11434/api/tags
```

### 2. Instalar modelo de embeddings (se necess√°rio)
```bash
docker exec ollama ollama pull nomic-embed-text
# ou alternativamente
docker exec ollama ollama pull llama2
```

### 3. Testar no navegador
1. Acesse: http://127.0.0.1:5500/test/test-embedding-service.html
2. Clique em "Verificar Status"
3. Clique em "Testar Ollama"
4. Se tudo estiver verde, teste a gera√ß√£o de embeddings

### 4. Testar via console
```javascript
// No console do navegador (index.html)
KC.EmbeddingService.checkOllamaAvailability()

// Gerar embedding
await KC.EmbeddingService.generateEmbedding(
    "Ambev implementou Machine Learning",
    { category: "IA" }
)

// Testar similaridade
const emb1 = await KC.EmbeddingService.generateEmbedding("IA no marketing")
const emb2 = await KC.EmbeddingService.generateEmbedding("Intelig√™ncia artificial em vendas")
const similarity = KC.EmbeddingService.cosineSimilarity(emb1.embedding, emb2.embedding)
console.log(`Similaridade: ${(similarity * 100).toFixed(2)}%`)
```

## üìä M√©tricas do Servi√ßo

O EmbeddingService rastreia:
- `generated`: Total de embeddings gerados
- `cached`: Total de hits no cache
- `errors`: Total de erros
- `cacheSize`: Tamanho atual do cache em mem√≥ria

## üéØ Pr√≥ximos Passos

### Imediato (hoje)
1. ‚úÖ Validar conex√£o com Ollama
2. ‚è≥ Testar com dados reais do sistema
3. ‚è≥ Verificar performance e ajustar cache

### Pr√≥xima sess√£o
1. [ ] Criar QdrantService.js para armazenamento vetorial
2. [ ] Integrar com RAGExportManager existente
3. [ ] Popular Qdrant com embeddings das categorias

## üí° Insights Importantes

### Decis√£o Pragm√°tica VPS
Seguindo o insight #170725-110245, estamos usando:
- **Ollama local**: Para embeddings r√°pidos (evita lat√™ncia)
- **VPS via Tailscale**: Para armazenamento (Qdrant, PostgreSQL)
- **Sem duplica√ß√£o**: Reutilizando infraestrutura existente

### Cache Inteligente
- IndexedDB para persist√™ncia entre sess√µes
- Cache em mem√≥ria para acesso ultra-r√°pido
- TTL de 7 dias (configur√°vel)
- Limite de 1000 itens em mem√≥ria

### Enriquecimento Contextual
Embeddings s√£o enriquecidos com:
- Categoria do arquivo
- Score de relev√¢ncia
- Tags personalizadas

Isso melhora a qualidade da busca sem√¢ntica posterior.

## üêõ Troubleshooting

### Ollama n√£o est√° dispon√≠vel
```bash
# Linux/Mac
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# Baixar de https://ollama.ai/download

# Iniciar servi√ßo
ollama serve
```

### Modelo n√£o encontrado
```bash
# Listar modelos dispon√≠veis
ollama list

# Instalar modelo de embeddings
ollama pull nomic-embed-text
```

### Cache n√£o funciona
- Verificar se o navegador suporta IndexedDB
- Limpar dados do site e recarregar
- Verificar console para erros

## ‚úÖ Checklist de Valida√ß√£o

- [x] EmbeddingService criado e documentado
- [x] Integrado no sistema (app.js e index.html)
- [x] P√°gina de testes funcional
- [ ] Ollama testado e funcionando
- [ ] Embeddings gerados com sucesso
- [ ] Cache validado (mem√≥ria e IndexedDB)
- [ ] Performance adequada (<100ms por embedding)

## üìù Notas

- O servi√ßo prioriza Ollama (local) para evitar custos e lat√™ncia
- Fallback autom√°tico para OpenAI se configurado
- Cache √© essencial para performance em produ√ß√£o
- Dimens√µes: 768 (modelo nomic-embed-text)

---

**Pr√≥xima atualiza√ß√£o**: Ap√≥s valida√ß√£o com dados reais do sistema